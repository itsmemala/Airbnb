{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('D:\\\\PGP-BABI\\\\Capstone\\\\Airbnb\\\\Tasmania-detailed listings.csv', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\malavika.suresh\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data_text = data[['description']]\n",
    "data_text['index'] = data_text.index\n",
    "data_text\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\malavika.suresh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\malavika.suresh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\malavika.suresh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.util import ngrams\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    text=repr(text) #Remove non-unicode characters\n",
    "    text=(text.encode('ascii','ignore')).decode(\"utf-8\")\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "def preprocess_bigram(text):\n",
    "    result = []\n",
    "    text=repr(text) #Remove non-unicode characters\n",
    "    text=((text.encode('ascii','ignore')).decode(\"utf-8\")).lower()\n",
    "    table=str.maketrans('', '', string.punctuation)\n",
    "    text = [w.translate(table) for w in text]\n",
    "    text = (''.join([w for w in text])).strip()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)  \n",
    "    text = [w for w in word_tokens if not w in stop_words]\n",
    "    text = (''.join([w+' ' for w in text])).strip()\n",
    "    for token in ngrams(text.split(), 2):\n",
    "        bigram = (''.join([w+' ' for w in token])).strip()\n",
    "        result.append(bigram)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'on', \"you'll\", 'whom', 'wouldn', \"isn't\", \"needn't\", 'does', 'above', 'being', 'these', 'each', 'itself', 'some', 'by', 've', 'wasn', 're', 'weren', 'most', \"shouldn't\", 'y', \"wouldn't\", 'of', \"you're\", \"it's\", 'themselves', 'from', 'with', 'where', 'am', 'then', 'my', 't', 'that', 'are', 'other', 'won', \"hasn't\", 'before', 'is', 'ma', \"doesn't\", 'all', 'hasn', 'haven', 'doing', 'until', 'him', 'isn', 'more', \"weren't\", 'we', 'i', 'yours', 'so', 'his', 'the', 'further', 'our', 'their', 'having', 'very', 'o', 'mightn', 'have', 'had', 'not', 'or', \"couldn't\", \"shan't\", 'below', 'she', 'once', 'in', 'any', 'it', 'if', 'your', 'off', 'too', 'hadn', \"don't\", \"you'd\", 'can', \"didn't\", \"hadn't\", 'who', 'down', 'me', 'how', 'll', 'only', 'yourselves', \"that'll\", 'while', 'herself', 'shouldn', 'than', 'same', 'be', 'up', 'again', \"aren't\", 'did', 'do', 'no', \"won't\", 'for', \"she's\", 'mustn', 'don', 'd', 'which', 'during', 'those', 'as', 'ours', 'should', 'between', 'own', 'just', 'yourself', \"mustn't\", 'to', 'been', 'they', 'was', 'a', 'when', 'aren', 'such', 'were', 'because', 'into', 'hers', 'will', \"should've\", 'why', 'nor', 'theirs', 'under', 'through', 'needn', \"you've\", 'out', 'now', 'ourselves', 'm', 'about', 'couldn', 'at', 'he', 'here', 's', \"mightn't\", 'what', 'you', 'himself', 'its', 'shan', 'both', 'ain', 'against', 'has', 'her', 'after', 'few', 'didn', 'an', 'there', 'this', 'doesn', \"haven't\", 'myself', 'over', \"wasn't\", 'and', 'them', 'but'}\n"
     ]
    }
   ],
   "source": [
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['This', 'renovated', '1885', 'house', 'has', 'fantastic', 'views', '&', 'is', 'an', 'easy', 'walk', 'to', 'city', 'centre,', 'or', 'a', 'stroll', 'to', 'the', 'North', 'Hobart', 'cafe/restaurant', 'strip.', '', 'The', 'renowned', 'Hill', 'Street', 'Grocer', 'is', 'a', '5', 'minute', 'walk', 'if', 'you', 'need', 'to', 'stock', 'up', 'on', 'quality', 'foodstuffs.', 'Smolt', 'Kitchen,', 'Pigeon', 'Whole', 'Bakery', 'Cafe', 'and', 'Room', 'for', 'a', 'Pony', 'are', 'all', 'great', 'breakfast', 'places', 'and', 'just', 'a', 'short', 'stroll', 'away.', 'It', 'is', 'a', 'vegetarian', 'household,', 'so', 'if', \"you're\", 'a', 'carnivore', \"you'll\", 'need', 'to', 'engage', 'your', 'inner', 'vego', 'while', 'at', 'home', ':-)', 'The', 'house', 'has', 'been', 'stylishly', 'renovated,', 'is', 'fully', 'insulated', 'and', 'warm', 'in', 'winter.', 'You', 'room', 'has', \"it's\", 'own', 'TV,', 'complete', 'with', 'chromecast,', 'so', 'you', 'can', 'watch', 'TV', 'in', 'your', 'room', 'if', 'I', 'am', 'already', 'using', 'the', 'living', 'room.', '', 'Your', 'room', 'is', 'downstairs', 'and', 'at', 'the', 'front', 'of', 'the', 'house', 'and', 'my', 'bedroom', 'is', 'upstairs', 'at', 'the', 'back', 'of', 'the', 'house,', 'so', 'we', 'have', 'our', 'privacy.', 'Your', 'room', 'also', 'has', 'a', 'TV', 'and', 'desk,', 'so', 'you', 'can', 'relax', 'in', 'the', 'evening', 'or', 'do', 'some', 'work!', 'There', 'is', 'only', 'one', 'bathroom,', 'but', 'there', 'are', 'two', 'separate', 'toilets', '(one', 'off', 'my', 'studio', 'in', 'the', 'basement),', 'and', 'the', 'bathroom']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['renov', 'hous', 'fantast', 'view', 'easi', 'walk', 'citi', 'centr', 'stroll', 'north', 'hobart', 'cafe', 'restaur', 'strip', 'renown', 'hill', 'street', 'grocer', 'minut', 'walk', 'need', 'stock', 'qualiti', 'foodstuff', 'smolt', 'kitchen', 'pigeon', 'bakeri', 'cafe', 'room', 'poni', 'great', 'breakfast', 'place', 'short', 'stroll', 'away', 'vegetarian', 'household', 'carnivor', 'need', 'engag', 'inner', 'vego', 'home', 'hous', 'stylishli', 'renov', 'fulli', 'insul', 'warm', 'winter', 'room', 'complet', 'chromecast', 'watch', 'room', 'live', 'room', 'room', 'downstair', 'hous', 'bedroom', 'upstair', 'hous', 'privaci', 'room', 'desk', 'relax', 'even', 'work', 'bathroom', 'separ', 'toilet', 'studio', 'basement', 'bathroom']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = documents[documents['index'] == 0].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['described latest', 'latest lonely', 'lonely planet', 'planet guide', 'guide tasmania', 'tasmania must', 'must stay', 'stay modern', 'modern restoration', 'restoration character', 'character cottage', 'cottage created', 'created fully', 'fully self', 'self contained', 'contained home', 'home provides', 'provides perfect', 'perfect base', 'base discovering', 'discovering hobart', 'hobart surrounds', 'surrounds warm', 'warm cosy', 'cosy winter', 'winter cool', 'cool summer', 'summer alfresco', 'alfresco dining', 'dining private', 'private courtyard', 'courtyard welcome', 'welcome quayle', 'quayle terrace', 'terrace looking', 'looking amazing', 'amazing stay', 'stay serene', 'serene sandy', 'sandy bay', 'bay location', 'location house', 'house within', 'within easy', 'easy walking', 'walking distance', 'distance attractions', 'attractions including', 'including salamanca', 'salamanca market', 'market mona', 'mona ferry', 'ferry terminal', 'terminal quayle', 'quayle terrace', 'terrace home', 'home away', 'away home', 'home beautiful', 'beautiful property', 'property many', 'many great', 'great features', 'features rustic', 'rustic stone', 'stone walled', 'walled courtyard', 'courtyard french', 'french doors', 'doors open', 'open onto', 'onto courtyard', 'courtyard modern', 'modern kitchen', 'kitchen modern', 'modern bathroom', 'bathroom winter', 'winter floor', 'floor heating', 'heating cosy', 'cosy gas', 'gas log', 'log fire', 'fire heat', 'heat pump', 'pump cold', 'cold seasons', 'seasons queen', 'queen sized', 'sized bed', 'bed main', 'main bedroom', 'bedroom double', 'double bed', 'bed second', 'second bedroom', 'bedroom cottage', 'cottage prime', 'prime location', 'location within', 'within 15', '15 minute', 'minute walk', 'walk salamanca', 'salamanca place', 'place hobart', 'hobart wa']\n"
     ]
    }
   ],
   "source": [
    "test_text=documents['description'][1]\n",
    "print(preprocess_bigram(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [renov, hous, fantast, view, easi, walk, citi,...\n",
       "1    [describ, latest, lone, planet, guid, tasmania...\n",
       "2    [kentisburi, countri, hous, gorgeou, self, cat...\n",
       "3    [storey, terrac, hous, locat, hobart, mere, mi...\n",
       "4    [bradman, hous, polish, board, modern, kitchen...\n",
       "5    [ideal, suit, coupl, coupl, accommod, accommod...\n",
       "6    [welcom, eagl, rise, mother, name, home, base,...\n",
       "7    [anderson, suit, queen, suit, stanley, privat,...\n",
       "8    [larg, work, urban, studio, rural, set, come, ...\n",
       "9    [room, brand, feder, style, hous, walk, love, ...\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs = documents['description'].map(preprocess)\n",
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [renovated 1885, 1885 house, house fantastic, ...\n",
       "1    [described latest, latest lonely, lonely plane...\n",
       "2    [kentisbury country, country house, house gorg...\n",
       "3    [two storey, storey 1880s, 1880s terrace, terr...\n",
       "4    [bradman house, house polished, polished board...\n",
       "5    [ideally suited, suited couples, couples two, ...\n",
       "6    [welcome eagles, eagles rise, rise mother, mot...\n",
       "7    [anderson suite, suite one, one five, five que...\n",
       "8    [large working, working urban, urban studio, s...\n",
       "9    [room brand, brand new, new federation, federa...\n",
       "Name: description, dtype: object"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_bigrams = documents['description'].map(preprocess_bigram)\n",
    "processed_bigrams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 away\n",
      "1 bakeri\n",
      "2 basement\n",
      "3 bathroom\n",
      "4 bedroom\n",
      "5 breakfast\n",
      "6 cafe\n",
      "7 carnivor\n",
      "8 centr\n",
      "9 chromecast\n",
      "10 citi\n"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "#Remove numbers ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1885 house\n",
      "1 5 minute\n",
      "2 already using\n",
      "3 also tv\n",
      "4 away vegetarian\n",
      "5 back house\n",
      "6 bakery cafe\n",
      "7 basement bathroom\n",
      "8 bathroom two\n",
      "9 bedroom upstairs\n",
      "10 breakfast places\n"
     ]
    }
   ],
   "source": [
    "dictionary_bigram = gensim.corpora.Dictionary(processed_bigrams)\n",
    "count = 0\n",
    "for k, v in dictionary_bigram.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "#Remove numbers ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_bigram.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 1),\n",
       " (5, 1),\n",
       " (6, 2),\n",
       " (13, 1),\n",
       " (17, 2),\n",
       " (18, 1),\n",
       " (20, 1),\n",
       " (22, 1),\n",
       " (24, 1),\n",
       " (26, 1),\n",
       " (33, 1),\n",
       " (45, 1),\n",
       " (49, 1),\n",
       " (58, 1),\n",
       " (73, 1),\n",
       " (75, 1),\n",
       " (79, 2),\n",
       " (82, 1),\n",
       " (84, 1),\n",
       " (86, 1),\n",
       " (97, 1),\n",
       " (98, 3),\n",
       " (126, 1),\n",
       " (131, 1),\n",
       " (136, 2),\n",
       " (158, 2),\n",
       " (173, 2),\n",
       " (179, 1),\n",
       " (197, 1),\n",
       " (218, 1),\n",
       " (223, 1),\n",
       " (224, 1),\n",
       " (245, 1),\n",
       " (270, 1),\n",
       " (285, 1),\n",
       " (306, 1),\n",
       " (307, 1),\n",
       " (311, 1),\n",
       " (315, 1),\n",
       " (350, 1),\n",
       " (352, 1),\n",
       " (375, 1),\n",
       " (382, 1),\n",
       " (388, 1),\n",
       " (395, 1),\n",
       " (396, 1),\n",
       " (403, 1),\n",
       " (405, 1),\n",
       " (466, 1),\n",
       " (478, 3),\n",
       " (503, 1),\n",
       " (509, 1),\n",
       " (534, 3),\n",
       " (552, 1),\n",
       " (555, 1),\n",
       " (598, 1),\n",
       " (623, 1),\n",
       " (645, 1),\n",
       " (656, 1),\n",
       " (699, 1),\n",
       " (765, 1),\n",
       " (861, 1),\n",
       " (870, 1),\n",
       " (895, 1),\n",
       " (916, 1),\n",
       " (974, 1),\n",
       " (1023, 1),\n",
       " (1117, 1),\n",
       " (1237, 1),\n",
       " (1402, 1),\n",
       " (1515, 2),\n",
       " (1516, 1),\n",
       " (1519, 1),\n",
       " (1779, 1)]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 1),\n",
       " (50, 1),\n",
       " (168, 1),\n",
       " (221, 1),\n",
       " (252, 1),\n",
       " (253, 1),\n",
       " (255, 1),\n",
       " (264, 1),\n",
       " (265, 1),\n",
       " (368, 1),\n",
       " (401, 1),\n",
       " (477, 1),\n",
       " (481, 1),\n",
       " (483, 1),\n",
       " (487, 1),\n",
       " (555, 1),\n",
       " (557, 1),\n",
       " (565, 1),\n",
       " (616, 1),\n",
       " (677, 1),\n",
       " (823, 1),\n",
       " (956, 2),\n",
       " (1014, 1),\n",
       " (1346, 1),\n",
       " (1533, 1),\n",
       " (1536, 1),\n",
       " (1554, 1),\n",
       " (1681, 1),\n",
       " (1722, 1),\n",
       " (2065, 1),\n",
       " (2139, 1),\n",
       " (2221, 1),\n",
       " (2344, 1),\n",
       " (2406, 1),\n",
       " (2417, 1),\n",
       " (2456, 1),\n",
       " (2633, 1),\n",
       " (2726, 1)]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus_bigram = [dictionary_bigram.doc2bow(doc) for doc in processed_bigrams]\n",
    "bow_corpus_bigram[4310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 2 (\"bathroom\") appears 1 time.\n",
      "Word 5 (\"centr\") appears 1 time.\n",
      "Word 6 (\"citi\") appears 2 time.\n",
      "Word 13 (\"fulli\") appears 1 time.\n",
      "Word 17 (\"hobart\") appears 2 time.\n",
      "Word 18 (\"home\") appears 1 time.\n",
      "Word 20 (\"inner\") appears 1 time.\n",
      "Word 22 (\"kitchen\") appears 1 time.\n",
      "Word 24 (\"minut\") appears 1 time.\n",
      "Word 26 (\"north\") appears 1 time.\n",
      "Word 33 (\"restaur\") appears 1 time.\n",
      "Word 45 (\"view\") appears 1 time.\n",
      "Word 49 (\"work\") appears 1 time.\n",
      "Word 58 (\"cool\") appears 1 time.\n",
      "Word 73 (\"heat\") appears 1 time.\n",
      "Word 75 (\"locat\") appears 1 time.\n",
      "Word 79 (\"modern\") appears 2 time.\n",
      "Word 82 (\"perfect\") appears 1 time.\n",
      "Word 84 (\"privat\") appears 1 time.\n",
      "Word 86 (\"provid\") appears 1 time.\n",
      "Word 97 (\"size\") appears 1 time.\n",
      "Word 98 (\"stay\") appears 3 time.\n",
      "Word 126 (\"guest\") appears 1 time.\n",
      "Word 131 (\"mountain\") appears 1 time.\n",
      "Word 136 (\"park\") appears 2 time.\n",
      "Word 158 (\"cook\") appears 2 time.\n",
      "Word 173 (\"microwav\") appears 2 time.\n",
      "Word 179 (\"storey\") appears 1 time.\n",
      "Word 197 (\"hide\") appears 1 time.\n",
      "Word 218 (\"access\") appears 1 time.\n",
      "Word 223 (\"entranc\") appears 1 time.\n",
      "Word 224 (\"fridg\") appears 1 time.\n",
      "Word 245 (\"enjoy\") appears 1 time.\n",
      "Word 270 (\"control\") appears 1 time.\n",
      "Word 285 (\"remot\") appears 1 time.\n",
      "Word 306 (\"set\") appears 1 time.\n",
      "Word 307 (\"stair\") appears 1 time.\n",
      "Word 311 (\"urban\") appears 1 time.\n",
      "Word 315 (\"brand\") appears 1 time.\n",
      "Word 350 (\"natur\") appears 1 time.\n",
      "Word 352 (\"offer\") appears 1 time.\n",
      "Word 375 (\"long\") appears 1 time.\n",
      "Word 382 (\"airbnb\") appears 1 time.\n",
      "Word 388 (\"build\") appears 1 time.\n",
      "Word 395 (\"dryer\") appears 1 time.\n",
      "Word 396 (\"equip\") appears 1 time.\n",
      "Word 403 (\"number\") appears 1 time.\n",
      "Word 405 (\"phone\") appears 1 time.\n",
      "Word 466 (\"light\") appears 1 time.\n",
      "Word 478 (\"apart\") appears 3 time.\n",
      "Word 503 (\"arriv\") appears 1 time.\n",
      "Word 509 (\"design\") appears 1 time.\n",
      "Word 534 (\"secur\") appears 3 time.\n",
      "Word 552 (\"bustl\") appears 1 time.\n",
      "Word 555 (\"fit\") appears 1 time.\n",
      "Word 598 (\"washer\") appears 1 time.\n",
      "Word 623 (\"block\") appears 1 time.\n",
      "Word 645 (\"fabul\") appears 1 time.\n",
      "Word 656 (\"window\") appears 1 time.\n",
      "Word 699 (\"purpos\") appears 1 time.\n",
      "Word 765 (\"dishwash\") appears 1 time.\n",
      "Word 861 (\"scene\") appears 1 time.\n",
      "Word 870 (\"load\") appears 1 time.\n",
      "Word 895 (\"oven\") appears 1 time.\n",
      "Word 916 (\"mind\") appears 1 time.\n",
      "Word 974 (\"split\") appears 1 time.\n",
      "Word 1023 (\"intern\") appears 1 time.\n",
      "Word 1117 (\"contact\") appears 1 time.\n",
      "Word 1237 (\"code\") appears 1 time.\n",
      "Word 1402 (\"prior\") appears 1 time.\n",
      "Word 1515 (\"garag\") appears 2 time.\n",
      "Word 1516 (\"multipl\") appears 1 time.\n",
      "Word 1519 (\"receiv\") appears 1 time.\n",
      "Word 1779 (\"oasi\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 4 (\"city centre\") appears 1 time.\n",
      "Word 50 (\"modern bathroom\") appears 1 time.\n",
      "Word 168 (\"fridge microwave\") appears 1 time.\n",
      "Word 221 (\"brand new\") appears 1 time.\n",
      "Word 252 (\"equipped kitchen\") appears 1 time.\n",
      "Word 253 (\"fully equipped\") appears 1 time.\n",
      "Word 255 (\"hidden airbnb\") appears 1 time.\n",
      "Word 264 (\"number hidden\") appears 1 time.\n",
      "Word 265 (\"phone number\") appears 1 time.\n",
      "Word 368 (\"provided guests\") appears 1 time.\n",
      "Word 401 (\"full size\") appears 1 time.\n",
      "Word 477 (\"5 minutes\") appears 1 time.\n",
      "Word 481 (\"centre 5\") appears 1 time.\n",
      "Word 483 (\"mountain views\") appears 1 time.\n",
      "Word 487 (\"purpose built\") appears 1 time.\n",
      "Word 555 (\"inner city\") appears 1 time.\n",
      "Word 557 (\"minutes walk\") appears 1 time.\n",
      "Word 565 (\"walk north\") appears 1 time.\n",
      "Word 616 (\"secure parking\") appears 1 time.\n",
      "Word 677 (\"2 storey\") appears 1 time.\n",
      "Word 823 (\"2 x\") appears 1 time.\n",
      "Word 956 (\"cook top\") appears 2 time.\n",
      "Word 1014 (\"kitchen full\") appears 1 time.\n",
      "Word 1346 (\"natural light\") appears 1 time.\n",
      "Word 1533 (\"prior arrival\") appears 1 time.\n",
      "Word 1536 (\"top oven\") appears 1 time.\n",
      "Word 1554 (\"offers private\") appears 1 time.\n",
      "Word 1681 (\"north hobarts\") appears 1 time.\n",
      "Word 1722 (\"please make\") appears 1 time.\n",
      "Word 2065 (\"microwave dishwasher\") appears 1 time.\n",
      "Word 2139 (\"parking 2\") appears 1 time.\n",
      "Word 2221 (\"size fridge\") appears 1 time.\n",
      "Word 2344 (\"key box\") appears 1 time.\n",
      "Word 2406 (\"make home\") appears 1 time.\n",
      "Word 2417 (\"modern 2\") appears 1 time.\n",
      "Word 2456 (\"home enjoy\") appears 1 time.\n",
      "Word 2633 (\"stay us\") appears 1 time.\n",
      "Word 2726 (\"enjoy apartment\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_4310 = bow_corpus_bigram[4310]\n",
    "for i in range(len(bow_doc_4310)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "                                               dictionary_bigram[bow_doc_4310[i][0]], \n",
    "bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0723285486791226),\n",
      " (1, 0.1843572883140609),\n",
      " (2, 0.08752667157025729),\n",
      " (3, 0.09020573798390472),\n",
      " (4, 0.1746716887977508),\n",
      " (5, 0.0948815748041038),\n",
      " (6, 0.07460829674321322),\n",
      " (7, 0.12384325181100403),\n",
      " (8, 0.18769064690090353),\n",
      " (9, 0.14236094523162135),\n",
      " (10, 0.10458545139524719),\n",
      " (11, 0.16051053826365885),\n",
      " (12, 0.164126257551869),\n",
      " (13, 0.0697084822117669),\n",
      " (14, 0.07617340288658488),\n",
      " (15, 0.20822042187337866),\n",
      " (16, 0.13696634793510562),\n",
      " (17, 0.053351259605510584),\n",
      " (18, 0.05132906294866642),\n",
      " (19, 0.17411059235658236),\n",
      " (20, 0.1941247631998289),\n",
      " (21, 0.23419600881480973),\n",
      " (22, 0.03467933523990206),\n",
      " (23, 0.05289469882685426),\n",
      " (24, 0.045420862183476435),\n",
      " (25, 0.15901747353201623),\n",
      " (26, 0.09902591208604368),\n",
      " (27, 0.07079698125183191),\n",
      " (28, 0.11584746249454395),\n",
      " (29, 0.11865672591659568),\n",
      " (30, 0.06794581024480954),\n",
      " (31, 0.19299593798376674),\n",
      " (32, 0.19034655161832306),\n",
      " (33, 0.07071552678966797),\n",
      " (34, 0.2791950553916102),\n",
      " (35, 0.08802346276673267),\n",
      " (36, 0.08722216308970168),\n",
      " (37, 0.18900081205594507),\n",
      " (38, 0.07527301429280285),\n",
      " (39, 0.17009831802959552),\n",
      " (40, 0.24146724826720253),\n",
      " (41, 0.13030915301690377),\n",
      " (42, 0.25951509570731535),\n",
      " (43, 0.10689505525896241),\n",
      " (44, 0.12728836224869305),\n",
      " (45, 0.04049513251888449),\n",
      " (46, 0.11415701682014356),\n",
      " (47, 0.13280875974302886),\n",
      " (48, 0.14783322831760393),\n",
      " (49, 0.13967201317352088)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.13925320307246192),\n",
      " (1, 0.238137142837902),\n",
      " (2, 0.24248065866038462),\n",
      " (3, 0.24025458415245043),\n",
      " (4, 0.13241227428062083),\n",
      " (5, 0.20226017061386803),\n",
      " (6, 0.21985692574128743),\n",
      " (7, 0.19480205195149514),\n",
      " (8, 0.21985692574128743),\n",
      " (9, 0.1316949201284479),\n",
      " (10, 0.1083326985708226),\n",
      " (11, 0.1304673935834662),\n",
      " (12, 0.18778698894132714),\n",
      " (13, 0.21596785321354423),\n",
      " (14, 0.24482711864938878),\n",
      " (15, 0.22265782902503103),\n",
      " (16, 0.15642860597658673),\n",
      " (17, 0.23611822355907572),\n",
      " (18, 0.21596785321354423),\n",
      " (19, 0.25273968060650115),\n",
      " (20, 0.22265782902503103),\n",
      " (21, 0.1699748540264585),\n",
      " (22, 0.21239880707891456),\n",
      " (23, 0.2499387773227575)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf_bigram = models.TfidfModel(bow_corpus_bigram)\n",
    "corpus_tfidf_bigram = tfidf_bigram[bow_corpus_bigram]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf_bigram:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=5, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_bigram = gensim.models.LdaMulticore(bow_corpus_bigram, num_topics=5, id2word=dictionary_bigram, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.019*\"hous\" + 0.015*\"room\" + 0.012*\"view\" + 0.010*\"kitchen\" + 0.010*\"guest\" + 0.010*\"beauti\" + 0.009*\"bathroom\" + 0.009*\"enjoy\" + 0.009*\"area\" + 0.008*\"comfort\"\n",
      "Topic: 1 \n",
      "Words: 0.016*\"cottag\" + 0.011*\"area\" + 0.010*\"hobart\" + 0.009*\"locat\" + 0.009*\"minut\" + 0.009*\"kitchen\" + 0.009*\"hous\" + 0.008*\"home\" + 0.008*\"beauti\" + 0.008*\"bathroom\"\n",
      "Topic: 2 \n",
      "Words: 0.019*\"room\" + 0.015*\"hobart\" + 0.013*\"home\" + 0.012*\"kitchen\" + 0.011*\"locat\" + 0.010*\"guest\" + 0.009*\"area\" + 0.009*\"apart\" + 0.009*\"park\" + 0.009*\"bathroom\"\n",
      "Topic: 3 \n",
      "Words: 0.014*\"beach\" + 0.014*\"minut\" + 0.011*\"view\" + 0.011*\"enjoy\" + 0.010*\"guest\" + 0.010*\"home\" + 0.009*\"privat\" + 0.009*\"locat\" + 0.009*\"access\" + 0.008*\"hous\"\n",
      "Topic: 4 \n",
      "Words: 0.016*\"room\" + 0.011*\"kitchen\" + 0.010*\"park\" + 0.010*\"larg\" + 0.009*\"area\" + 0.009*\"hous\" + 0.009*\"apart\" + 0.008*\"bathroom\" + 0.008*\"view\" + 0.008*\"space\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.008*\"self contained\" + 0.007*\"open plan\" + 0.006*\"living area\" + 0.005*\"equipped kitchen\" + 0.005*\"fully equipped\" + 0.004*\"national park\" + 0.004*\"free wifi\" + 0.004*\"double bed\" + 0.004*\"tea coffee\" + 0.004*\"brand new\"\n",
      "Topic: 1 \n",
      "Words: 0.008*\"walking distance\" + 0.007*\"fully equipped\" + 0.007*\"street parking\" + 0.006*\"minutes walk\" + 0.006*\"equipped kitchen\" + 0.006*\"self contained\" + 0.005*\"minutes drive\" + 0.005*\"queen bed\" + 0.005*\"minute walk\" + 0.005*\"5 minutes\"\n",
      "Topic: 2 \n",
      "Words: 0.007*\"walking distance\" + 0.005*\"size bed\" + 0.005*\"washing machine\" + 0.005*\"phone number\" + 0.005*\"number hidden\" + 0.005*\"queen size\" + 0.005*\"tea coffee\" + 0.005*\"hidden airbnb\" + 0.004*\"wood fire\" + 0.004*\"fully equipped\"\n",
      "Topic: 3 \n",
      "Words: 0.008*\"minute drive\" + 0.007*\"open plan\" + 0.007*\"queen bed\" + 0.006*\"living area\" + 0.005*\"self contained\" + 0.005*\"bay fires\" + 0.005*\"free wifi\" + 0.004*\"walking distance\" + 0.004*\"10 minute\" + 0.004*\"minute walk\"\n",
      "Topic: 4 \n",
      "Words: 0.006*\"walking distance\" + 0.005*\"minute drive\" + 0.005*\"hidden airbnb\" + 0.005*\"website hidden\" + 0.005*\"living room\" + 0.005*\"queen bed\" + 0.005*\"cradle mountain\" + 0.005*\"queen size\" + 0.005*\"free wifi\" + 0.005*\"minute walk\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_bigram.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.005*\"cottag\" + 0.005*\"room\" + 0.004*\"apart\" + 0.004*\"hous\" + 0.004*\"minut\" + 0.003*\"beach\" + 0.003*\"place\" + 0.003*\"cradl\" + 0.003*\"view\" + 0.003*\"garden\"\n",
      "Topic: 1 Word: 0.005*\"room\" + 0.004*\"hobart\" + 0.004*\"apart\" + 0.004*\"cottag\" + 0.004*\"home\" + 0.004*\"hous\" + 0.004*\"area\" + 0.004*\"guest\" + 0.003*\"minut\" + 0.003*\"live\"\n",
      "Topic: 2 Word: 0.004*\"room\" + 0.004*\"hous\" + 0.004*\"beach\" + 0.004*\"cottag\" + 0.004*\"view\" + 0.004*\"guest\" + 0.004*\"apart\" + 0.003*\"home\" + 0.003*\"park\" + 0.003*\"area\"\n",
      "Topic: 3 Word: 0.005*\"beach\" + 0.005*\"island\" + 0.004*\"hous\" + 0.004*\"cottag\" + 0.004*\"hobart\" + 0.003*\"room\" + 0.003*\"garden\" + 0.003*\"minut\" + 0.003*\"apart\" + 0.003*\"min\"\n",
      "Topic: 4 Word: 0.007*\"apart\" + 0.005*\"hobart\" + 0.005*\"minut\" + 0.004*\"home\" + 0.004*\"room\" + 0.004*\"larg\" + 0.004*\"place\" + 0.004*\"hous\" + 0.004*\"beach\" + 0.003*\"street\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=5, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.003*\"walking distance\" + 0.003*\"open plan\" + 0.003*\"fully equipped\" + 0.003*\"minute walk\" + 0.002*\"bay fires\" + 0.002*\"minute drive\" + 0.002*\"east coast\" + 0.002*\"sandy bay\" + 0.002*\"equipped kitchen\" + 0.002*\"self contained\"\n",
      "Topic: 1 Word: 0.003*\"self contained\" + 0.003*\"walking distance\" + 0.003*\"good couples\" + 0.003*\"place good\" + 0.003*\"youll love\" + 0.003*\"free wifi\" + 0.002*\"min walk\" + 0.002*\"families kids\" + 0.002*\"love place\" + 0.002*\"tea coffee\"\n",
      "Topic: 2 Word: 0.003*\"fully equipped\" + 0.003*\"good couples\" + 0.003*\"place good\" + 0.002*\"solo adventurers\" + 0.002*\"equipped kitchen\" + 0.002*\"living area\" + 0.002*\"couples solo\" + 0.002*\"place close\" + 0.002*\"beach house\" + 0.002*\"minutes drive\"\n",
      "Topic: 3 Word: 0.004*\"self contained\" + 0.003*\"queen bed\" + 0.003*\"hidden airbnb\" + 0.003*\"minute walk\" + 0.002*\"queen size\" + 0.002*\"minutes drive\" + 0.002*\"open plan\" + 0.002*\"cradle mountain\" + 0.002*\"fully equipped\" + 0.002*\"size bed\"\n",
      "Topic: 4 Word: 0.003*\"equipped kitchen\" + 0.003*\"free wifi\" + 0.003*\"open plan\" + 0.003*\"queen bed\" + 0.002*\"hidden airbnb\" + 0.002*\"phone number\" + 0.002*\"fully equipped\" + 0.002*\"walking distance\" + 0.002*\"number hidden\" + 0.002*\"self contained\"\n",
      "Topic: 5 Word: 0.004*\"walking distance\" + 0.003*\"minute drive\" + 0.002*\"minute walk\" + 0.002*\"north hobart\" + 0.002*\"street parking\" + 0.002*\"within walking\" + 0.002*\"size bed\" + 0.002*\"away home\" + 0.002*\"ground floor\" + 0.002*\"queen size\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf_bigram = gensim.models.LdaMulticore(corpus_tfidf_bigram, num_topics=6, id2word=dictionary_bigram, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf_bigram.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
